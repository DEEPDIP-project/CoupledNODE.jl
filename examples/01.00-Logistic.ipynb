{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic equation and NODE\n",
    "Let's study a phenomenon that can be described with the following ODE $$\\dfrac{dP}{dt} = rP\\left(1-\\dfrac{P}{K}\\right),$$ which is called the logistic equation. Given $P(t=0)=P_0$ we can solve this problem analytically to get $P(t) = \\frac{K}{1+\\left(K-P_0\\right)/P_0 \\cdot e^{-rt}}$. Let's plot the solution for $r=K=2, P_0=0.01$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = 2\n",
    "K = 1\n",
    "P0 = 0.01\n",
    "function P(t)\n",
    "    return K / (1 + (K - P0) * exp(-r * t) / P0)\n",
    "end\n",
    "t = range(start = 0, stop = 6, step = 0.01)\n",
    "Pt = P.(t)\n",
    "using Plots\n",
    "plot(t, Pt, label = \"P(t)\", xlabel = \"t\", ylabel = \"P(t)\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's say that we want to use the logistic equation to model an experiment like the activation energy of a neuron. We run the experiment and we observe the following:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import DifferentialEquations: ODEProblem, solve, Tsit5\n",
    "function observation()\n",
    "    f_o(u, p, t) = u .* (0.0 .- 0.8 .* log.(u))\n",
    "    trange = (0.0, 6.0)\n",
    "    prob = ODEProblem(f_o, 0.01, trange, dt = 0.01, saveat = 0.01)\n",
    "    sol = solve(prob, Tsit5())\n",
    "    return sol.u\n",
    "end\n",
    "u_experiment = observation()\n",
    "using Plots\n",
    "plot(t, Pt, label = \"Best P(t) fit\")\n",
    "plot!(t, u_experiment[:], label = \"Observation(t)\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This means that our experimental system, despite its similarity, it is not described by a logistic ODE.\n",
    "How can we then model our experiment as an ODE?\n",
    "\n",
    "There are many simpler alternatives for this example (e.g. Mode Decomposition, SINDY or Bayesian methods), but let's use this exercise to introduce a NODE:\n",
    "$\\begin{equation}\\dfrac{du}{dt} = \\underbrace{ru\\left(1-\\dfrac{u}{K}\\right)}_{f(u)} + NN(u|\\theta).\\end{equation}$\n",
    "In this NODE we are looking for a solution $u(t)$ that reproduces our observation.\n",
    "We will be using [SciML](https://sciml.ai/) package [DiffEqFlux.jl](https://github.com/SciML/DiffEqFlux.jl) and scpecifically [NeuralODE](https://docs.sciml.ai/DiffEqFlux/stable/examples/neural_ode/) for defining and solving the problem.\n",
    "## Solve the NODE\n",
    "We solve this 1D NODE using introducing the functionalities of this repository:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We create the NN using `Lux`. In this example we do not discuss the structure of the NN, but we leave it as a black box that can be designed by the user. We will show later how to take inspiration from the physics of the problem to design optimal NN."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Lux\n",
    "NN = Lux.Chain(Lux.SkipConnection(Lux.Dense(1, 3),\n",
    "    (out, u) -> u .* out[1] .+ u .* u .* out[2] .+ u .* log.(abs.(u)) .* out[3]));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We define the force $f(u)$ compatibly with SciML."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f_u(u) = @. r * u * (1.0 - u / K);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We create a spatial grid. In this case we have a 1D problem with a single point."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import CoupledNODE: create_f_CNODE, Grid\n",
    "grid_u = Grid(dim = 1, dx = 1.0, nx = 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the model\n",
    "We define parameters necessary for the solution such as time step and how often to save the solutions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "dt = 0.01 # it has to be as fine as the data\n",
    "nunroll = 60\n",
    "t_range = (0.0, 6.0) # it has to be as long as unroll"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define our model `NeuralODE` with all the elements that we have defined until now."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "full_NODE = create_f_CNODE((f_u,), (grid_u,), t_range, Tsit5(), (NN,); adaptive = false,\n",
    "    dt = 0.001, saveat = 0.2, is_closed = true);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and get the parameters that you want to train"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Random\n",
    "rng = Random.seed!(1234)\n",
    "θ, st = Lux.setup(rng, full_NODE.model);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We solve the NODE using the zero-initialized parameters to get a reference solution (an untrained one).\n",
    "*Note:* `full_NODE` is a `NeuralODE` function that returns a `DiffEqBase.ODESolution` object. This object contains the solution of the ODE, but it also contains additional information like the time points at which the solution was evaluated and the parameters of the ODE. We can access the solution using `[1]`, and we convert it to an `Array` to be able to use it for further calculations and plot. In this case we get the untrained solution for reference."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "untrained_NODE_solution = Array(full_NODE(hcat([u_experiment[1]]), θ, st)[1]);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We reshape the data (labels) to be compatible with the loss function. We want a vector with shape (dim_u, n_samples, t_steps)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "u_experiment_mod = reshape(u_experiment, grid_u.nx, 1, length(u_experiment))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create an auxiliary `training_NODE` thatis in principle the same as `full_NODE`, but it is used to train the NODE with different time steps and different number of samples."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "dt = 0.01 # it has to be as fine as the data\n",
    "nunroll = 60\n",
    "t_train_range = (0.0, dt * nunroll) # it has to be as long as unroll\n",
    "training_NODE = create_f_CNODE((f_u,), (grid_u,), t_train_range, Tsit5(), (NN,);\n",
    "    adaptive = false, dt = dt, saveat = dt, is_closed = true);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We design the **loss function**. For this example, we use *multishooting a posteriori* fitting [(MulDtO)](https://docs.sciml.ai/DiffEqFlux/dev/examples/multiple_shooting/). Using `Zygote` we compare `nintervals` of length `nunroll` to get the gradient. Notice that this method is differentiating through the solution of the NODE!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import CoupledNODE: create_randloss_MulDtO\n",
    "myloss = create_randloss_MulDtO(\n",
    "    u_experiment_mod, training_NODE, st, nunroll = nunroll, nintervals = 5, nsamples = 1);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize and trigger the compilation of the model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import ComponentArrays\n",
    "pinit = ComponentArrays.ComponentArray(θ);\n",
    "myloss(pinit); # trigger compilation"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the autodifferentiation type"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import OptimizationOptimisers: Optimization\n",
    "adtype = Optimization.AutoZygote();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We transform the NeuralODE into an optimization problem"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "optf = Optimization.OptimizationFunction((x, p) -> myloss(x), adtype);\n",
    "optprob = Optimization.OptimizationProblem(optf, pinit);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the training algorithm:\n",
    "Adam with learning rate 0.01, with gradient clipping"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import OptimizationOptimisers: OptimiserChain, Adam, ClipGrad\n",
    "algo = OptimiserChain(Adam(1.0e-2), ClipGrad(1));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or this other optimizer (uncomment tu use)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#import OptimizationOptimJL: Optim\n",
    "#algo = Optim.LBFGS();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train de NODE\n",
    "We are ready to train the NODE.\n",
    "Notice that the block can be repeated to continue training"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import CoupledNODE: callback\n",
    "result_training = Optimization.solve(optprob,\n",
    "    algo;\n",
    "    callback = callback,\n",
    "    maxiters = 1000)\n",
    "pinit = result_training.u; #optimized params\n",
    "optprob = Optimization.OptimizationProblem(optf, pinit);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we get our results, notice that we use `full_NODE` instead of `training_NODE` so that it will plot as expected. Also notice that we use the optimized parameters `pinit`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "trained_NODE_solution = Array(full_NODE(hcat([u_experiment[1]]), pinit, st)[1]);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse the results\n",
    "Visualize the obtained results"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(t, Pt, label = \"Best P(t) fit\")\n",
    "plot!(t, u_experiment[:], label = \"Observation(t)\")\n",
    "scatter!(range(start = 0, stop = 6, step = 0.2),\n",
    "    untrained_NODE_solution[:],\n",
    "    label = \"untrained NODE\",\n",
    "    marker = :circle)\n",
    "scatter!(range(start = 0, stop = 6, step = 0.2),\n",
    "    trained_NODE_solution[:],\n",
    "    label = \"Trained NODE\",\n",
    "    marker = :circle)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
