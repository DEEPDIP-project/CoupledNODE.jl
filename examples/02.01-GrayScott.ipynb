{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Learning the Gray-Scott model: a priori fitting\n",
    "In the previous example [02.00-GrayScott.jl](02.00-GrayScott.jl) we have seen how to use the CNODE to solve the Gray-Scott model via an explicit method.\n",
    "Here we introduce the *Learning part* of the CNODE, and show how it can be used to close the CNODE. We are going to train the neural network via **a priori fitting** and in the next example [02.02-GrayScott](02.02-GrayScott.jl) we will show how to use a posteriori fitting."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a reminder, the GS model is defined from\n",
    "$\\begin{equation}\\begin{cases} \\frac{du}{dt} = D_u \\Delta u - uv^2 + f(1-u)  \\equiv F_u(u,v) \\\\ \\frac{dv}{dt} = D_v \\Delta v + uv^2 - (f+k)v  \\equiv G_v(u,v)\\end{cases} \\end{equation}$\n",
    "where $u(x,y,t):\\mathbb{R}^2\\times \\mathbb{R}\\rightarrow \\mathbb{R}$ is the concentration of species 1, while $v(x,y,t)$ is the concentration of species two. This model reproduce the effect of the two species diffusing in their environment, and reacting together.\n",
    "This effect is captured by the ratios between $D_u$ and $D_v$ (diffusion coefficients) and $f$ and $k$ (reaction rates)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we will first (I) use the exact GS model to gather some data\n",
    "then in the second part (II), we will train a generic CNODE to show that it can learn the GS model from the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I. Solving GS to collect data\n",
    "Definition of the grid"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import CoupledNODE: Grid\n",
    "dux = duy = dvx = dvy = 1.0\n",
    "nux = nuy = nvx = nvy = 64\n",
    "grid_GS_u = Grid(dim = 2, dx = dux, nx = nux, dy = duy, ny = nuy)\n",
    "grid_GS_v = Grid(dim = 2, dx = dvx, nx = nvx, dy = dvy, ny = nvy)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definition of the initial condition as a random perturbation over a constant background to add variety.\n",
    "Notice that this initial conditions are different from those of the previous example."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Random\n",
    "function initial_condition(grid_u, grid_v, U₀, V₀, ε_u, ε_v; nsimulations = 1)\n",
    "    u_init = U₀ .+ ε_u .* Random.randn(grid_u.nx, grid_u.ny, nsimulations)\n",
    "    v_init = V₀ .+ ε_v .* Random.randn(grid_v.nx, grid_v.ny, nsimulations)\n",
    "    return u_init, v_init\n",
    "end\n",
    "U₀ = 0.5    # initial concentration of u\n",
    "V₀ = 0.25   # initial concentration of v\n",
    "ε_u = 0.05  # magnitude of the perturbation on u\n",
    "ε_v = 0.1   # magnitude of the perturbation on v\n",
    "u_initial, v_initial = initial_condition(\n",
    "    grid_GS_u, grid_GS_v, U₀, V₀, ε_u, ε_v, nsimulations = 20);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define the initial condition as a flattened concatenated array"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "uv0 = vcat(reshape(u_initial, grid_GS_u.N, :), reshape(v_initial, grid_GS_v.N, :));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "These are the GS parameters (also used in example 02.01) that we will try to learn"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "D_u = 0.16\n",
    "D_v = 0.08\n",
    "f = 0.055\n",
    "k = 0.062;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exact right hand sides (functions) of the GS model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import CoupledNODE: Laplacian\n",
    "F_u(u, v) = D_u * Laplacian(u, grid_GS_u.dx, grid_GS_u.dy) .- u .* v .^ 2 .+ f .* (1.0 .- u)\n",
    "G_v(u, v) = D_v * Laplacian(v, grid_GS_v.dx, grid_GS_v.dy) .+ u .* v .^ 2 .- (f + k) .* v"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definition of the CNODE"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Lux\n",
    "import CoupledNODE: create_f_CNODE\n",
    "f_CNODE = create_f_CNODE((F_u, G_v), (grid_GS_u, grid_GS_v); is_closed = false);\n",
    "rng = Random.seed!(1234);\n",
    "θ_0, st_0 = Lux.setup(rng, f_CNODE);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Burnout run:** to discard the results of the initial conditions.\n",
    "In this case we need 2 burnouts: first one with a relatively large time step and then another one with a smaller time step. This allow us to discard the transient dynamics and to have a good initial condition for the data collection run."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import DifferentialEquations: Tsit5\n",
    "import DiffEqFlux: NeuralODE\n",
    "trange_burn = (0.0, 1.0)\n",
    "dt, saveat = (1e-2, 1)\n",
    "burnout_CNODE = NeuralODE(f_CNODE,\n",
    "    trange_burn,\n",
    "    Tsit5(),\n",
    "    adaptive = false,\n",
    "    dt = dt,\n",
    "    saveat = saveat);\n",
    "burnout_CNODE_solution = Array(burnout_CNODE(uv0, θ_0, st_0)[1]);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second burnout with a smaller timestep"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "trange_burn = (0.0, 500.0)\n",
    "dt, saveat = (1 / (4 * max(D_u, D_v)), 100)\n",
    "burnout_CNODE = NeuralODE(f_CNODE,\n",
    "    trange_burn,\n",
    "    Tsit5(),\n",
    "    adaptive = false,\n",
    "    dt = dt,\n",
    "    saveat = saveat);\n",
    "burnout_CNODE_solution = Array(burnout_CNODE(burnout_CNODE_solution[:, :, end], θ_0, st_0)[1]);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data collection run"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "uv0 = burnout_CNODE_solution[:, :, end];\n",
    "trange = (0.0, 2000.0);\n",
    "dt, saveat = (1 / (4 * max(D_u, D_v)), 1);\n",
    "GS_CNODE = NeuralODE(f_CNODE, trange, Tsit5(), adaptive = false, dt = dt, saveat = saveat);\n",
    "GS_sim = Array(GS_CNODE(uv0, θ_0, st_0)[1]);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`GS_sim` contains the solutions of $u$ and $v$ for the specified `trange` and `nsimulations` initial conditions. If you explore `GS_sim` you will see that it has the shape `((nux * nuy) + (nvx * nvy), nsimulations, timesteps)`.\n",
    "`uv_data` is a reshaped version of `GS_sim` that has the shape `(nux * nuy + nvx * nvy, nsimulations * timesteps)`. This is the format that we will use to train the CNODE."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "uv_data = reshape(GS_sim, size(GS_sim, 1), size(GS_sim, 2) * size(GS_sim, 3));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define `FG_target` containing the right hand sides (i.e. $\\frac{du}{dt} and \\frac{dv}{dt}$) of each one of the samples. We will see later that for the training `FG_target` is used as the labels to do derivative fitting."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "FG_target = Array(f_CNODE(uv_data, θ_0, st_0)[1]);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II. Training a CNODE to learn the GS model via a priori training\n",
    "To learn the GS model, we will use the following CNODE\n",
    "\\begin{equation}\\begin{cases} \\frac{du}{dt} = D_u \\Delta u - uv^2 + \\theta_{u,1} u +\\theta_{u,2} v +\\theta_{u,3}  \\\\ \\frac{dv}{dt} = D_v \\Delta v + uv^2 + \\theta_{v,1} u +\\theta_{v,2} v +\\theta_{v,3} \\end{cases} \\end{equation}\n",
    "In this example the deterministic function contains the diffusion and the coupling terms, while the model has to learn the source and death terms.\n",
    "The deterministic functions of the two coupled equations are:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Zygote\n",
    "function F_u_open(u, v)\n",
    "    Zygote.@ignore D_u * Laplacian(u, grid_GS_u.dx, grid_GS_u.dy) .- u .* v .^ 2\n",
    "end;\n",
    "function G_v_open(u, v)\n",
    "    Zygote.@ignore D_v * Laplacian(v, grid_GS_v.dx, grid_GS_v.dy) .+ u .* v .^ 2\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We tell Zygote to ignore this tree branch for the gradient propagation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the trainable part, we define an abstract Lux layer"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct GSLayer{F} <: Lux.AbstractExplicitLayer\n",
    "    init_weight::F\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and its (outside) constructor"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function GSLayer(; init_weight = Lux.zeros32)\n",
    "    #function GSLayer(; init_weight = Lux.glorot_uniform)\n",
    "    return GSLayer(init_weight)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need to specify how to initialize its parameters and states.\n",
    "This custom layer does not have any hidden states (RNGs) that are modified."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function Lux.initialparameters(rng::Random.AbstractRNG, (; init_weight)::GSLayer)\n",
    "    (;\n",
    "        gs_weights = init_weight(rng, 3),)\n",
    "end\n",
    "Lux.initialstates(::Random.AbstractRNG, ::GSLayer) = (;)\n",
    "Lux.parameterlength((;)::GSLayer) = 3\n",
    "Lux.statelength(::GSLayer) = 0"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define how to pass inputs through `GSlayer`, assuming the following:\n",
    "- Input size: `(N, N, 2, nsample)`, where the two channels are $u$ and $v$.\n",
    "- Output size: `(N, N, nsample)` where we assumed monochannel output, so we dropped the channel dimension."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is what each layer does. Notice that the layer does not modify the state."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function ((;)::GSLayer)(x, params, state)\n",
    "    (u, v) = x\n",
    "    out = params.gs_weights[1] .* u .+ params.gs_weights[2] .* v .+ params.gs_weights[3]\n",
    "    out, state\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create the trainable models. In this case is just a GS layer, but the model can be as complex as needed."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "NN_u = GSLayer()\n",
    "NN_v = GSLayer()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now close the CNODE with the Neural Network"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f_closed_CNODE = create_f_CNODE(\n",
    "    (F_u_open, G_v_open), (grid_GS_u, grid_GS_v), (NN_u, NN_v); is_closed = true)\n",
    "θ, st = Lux.setup(rng, f_closed_CNODE);\n",
    "print(θ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check that the closed CNODE can reproduce the GS model if the parameters are set to the correct values"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import ComponentArrays\n",
    "correct_w_u = [-f, 0, f];\n",
    "correct_w_v = [0, -(f + k), 0];\n",
    "θ_correct = ComponentArrays.ComponentArray(θ)\n",
    "θ_correct.layer_2.layer_1.gs_weights = correct_w_u;\n",
    "θ_correct.layer_2.layer_2.gs_weights = correct_w_v;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that they are the same within a tolerance of 1e-7"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "isapprox(f_closed_CNODE(GS_sim[:, 1, 1], θ_correct, st)[1],\n",
    "    f_CNODE(GS_sim[:, 1, 1], θ_0, st_0)[1],\n",
    "    atol = 1e-7,\n",
    "    rtol = 1e-7)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "but now with a tolerance of 1e-8 this check returns `false`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "isapprox(f_closed_CNODE(GS_sim[:, 1, 1], θ_correct, st)[1],\n",
    "    f_CNODE(GS_sim[:, 1, 1], θ_0, st_0)[1],\n",
    "    atol = 1e-8,\n",
    "    rtol = 1e-8)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a chaotic system like GS, this would be enough to produce different dynamics, so be careful about this"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have problems with training the model, you can cheat and start from the solution to check your implementation:\n",
    "```julia\n",
    "θ.layer_2.layer_1.gs_weights = correct_w_u\n",
    "θ.layer_2.layer_2.gs_weights = correct_w_v\n",
    "pinit = θ\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Design the loss function - a priori fitting\n",
    "For this example, we use *a priori* fitting. In this approach, the loss function is defined to minimize the difference between the derivatives of $\\frac{du}{dt}$ and $\\frac{dv}{dt}$ predicted by the model and calculated via explicit method `FG_target`.\n",
    "In practice, we use [Zygote](https://fluxml.ai/Zygote.jl/stable/) to compare the right hand side of the GS model with the right hand side of the CNODE, and we ask it to minimize the difference."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import CoupledNODE: create_randloss_derivative\n",
    "myloss = create_randloss_derivative(uv_data,\n",
    "    FG_target,\n",
    "    f_closed_CNODE,\n",
    "    st;\n",
    "    n_use = 64,\n",
    "    λ = 0);\n",
    "\n",
    "# Initialize and trigger the compilation of the model\n",
    "pinit = ComponentArrays.ComponentArray(θ);\n",
    "myloss(pinit);\n",
    "# [!] Check that the loss does not get type warnings, otherwise it will be slower"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We transform the NeuralODE into an optimization problem"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Select the autodifferentiation type\n",
    "import OptimizationOptimisers: Optimization\n",
    "adtype = Optimization.AutoZygote();\n",
    "optf = Optimization.OptimizationFunction((x, p) -> myloss(x), adtype);\n",
    "optprob = Optimization.OptimizationProblem(optf, pinit);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the training algorithm:\n",
    "In the previous example we have used a classic gradient method like Adam:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import OptimizationOptimisers: OptimiserChain, Adam\n",
    "algo = OptimiserChain(Adam(1.0e-3));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "notice however that CNODEs can be trained with any Julia optimizer, including the ones from the `Optimization` package like LBFGS"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import OptimizationOptimJL: Optim\n",
    "algo = Optim.LBFGS();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "or even gradient-free methods like CMA-ES that we use for this example"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using OptimizationCMAEvolutionStrategy, Statistics\n",
    "algo = CMAEvolutionStrategyOpt();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the CNODE"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import CoupledNODE: callback\n",
    "result_neuralode = Optimization.solve(optprob,\n",
    "    algo;\n",
    "    callback = callback,\n",
    "    maxiters = 150);\n",
    "pinit = result_neuralode.u;\n",
    "θ = pinit;\n",
    "optprob = Optimization.OptimizationProblem(optf, pinit);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Notice that the block above can be repeated to continue training, however don't do that with CMA-ES since it will restart from a random initial population)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III. Analyse the results\n",
    "Let's compare the learned weights to the values that we expect"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Plots, Plots.PlotMeasures\n",
    "gs_w_u = θ.layer_2.layer_1.gs_weights;\n",
    "gs_w_v = θ.layer_2.layer_2.gs_weights;\n",
    "p1 = scatter(gs_w_u,\n",
    "    label = \"learned\",\n",
    "    title = \"Comparison NN_u coefficients\",\n",
    "    xlabel = \"Index\",\n",
    "    ylabel = \"Value\")\n",
    "scatter!(p1, correct_w_u, label = \"correct\")\n",
    "p2 = scatter(gs_w_v,\n",
    "    label = \"learned\",\n",
    "    title = \"Comparison NN_v coefficients\",\n",
    "    xlabel = \"Index\",\n",
    "    ylabel = \"Value\")\n",
    "scatter!(p2, correct_w_v, label = \"correct\")\n",
    "p = plot(p1, p2, layout = (2, 1))\n",
    "display(p)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The learned weights look perfect, but let's check what happens if we use them to solve the GS model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's solve the system, for two different set of parameters, with the trained CNODE and compare with the exact solution"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "trange = (0.0, 500);\n",
    "dt, saveat = (1, 5);\n",
    "\n",
    "# Exact solution\n",
    "f_exact = create_f_CNODE((F_u, G_v), (grid_GS_u, grid_GS_v); is_closed = false)\n",
    "θ_e, st_e = Lux.setup(rng, f_exact);\n",
    "exact_CNODE = NeuralODE(f_exact,\n",
    "    trange,\n",
    "    Tsit5(),\n",
    "    adaptive = false,\n",
    "    dt = dt,\n",
    "    saveat = saveat);\n",
    "exact_CNODE_solution = Array(exact_CNODE(GS_sim[:, 1:2, 1], θ_e, st_e)[1]);\n",
    "u = reshape(exact_CNODE_solution[1:(grid_GS_u.N), :, :],\n",
    "    grid_GS_u.nx,\n",
    "    grid_GS_u.ny,\n",
    "    size(exact_CNODE_solution, 2),\n",
    "    :);\n",
    "v = reshape(exact_CNODE_solution[(grid_GS_v.N + 1):end, :, :],\n",
    "    grid_GS_v.nx,\n",
    "    grid_GS_v.ny,\n",
    "    size(exact_CNODE_solution, 2),\n",
    "    :);\n",
    "\n",
    "# Trained solution\n",
    "trained_CNODE = NeuralODE(f_closed_CNODE,\n",
    "    trange,\n",
    "    Tsit5(),\n",
    "    adaptive = false,\n",
    "    dt = dt,\n",
    "    saveat = saveat);\n",
    "trained_CNODE_solution = Array(trained_CNODE(GS_sim[:, 1:3, 1], θ, st)[1]);\n",
    "u_trained = reshape(trained_CNODE_solution[1:(grid_GS_u.N), :, :],\n",
    "    grid_GS_u.nx,\n",
    "    grid_GS_u.ny,\n",
    "    size(trained_CNODE_solution, 2),\n",
    "    :);\n",
    "v_trained = reshape(trained_CNODE_solution[(grid_GS_u.N + 1):end, :, :],\n",
    "    grid_GS_v.nx,\n",
    "    grid_GS_v.ny,\n",
    "    size(trained_CNODE_solution, 2),\n",
    "    :);\n",
    "f_u = create_f_CNODE(\n",
    "    (F_u_open, G_v_open), (grid_GS_u, grid_GS_v), (NN_u, NN_v); is_closed = false)\n",
    "θ_u, st_u = Lux.setup(rng, f_u);\n",
    "\n",
    "# Untrained solution\n",
    "untrained_CNODE = NeuralODE(f_u,\n",
    "    trange,\n",
    "    Tsit5(),\n",
    "    adaptive = false,\n",
    "    dt = dt,\n",
    "    saveat = saveat);\n",
    "untrained_CNODE_solution = Array(untrained_CNODE(GS_sim[:, 1:3, 1], θ_u, st_u)[1]);\n",
    "u_untrained = reshape(untrained_CNODE_solution[1:(grid_GS_u.N), :, :],\n",
    "    grid_GS_u.nx,\n",
    "    grid_GS_u.ny,\n",
    "    size(untrained_CNODE_solution, 2),\n",
    "    :);\n",
    "v_untrained = reshape(untrained_CNODE_solution[(grid_GS_v.N + 1):end, :, :],\n",
    "    grid_GS_v.nx,\n",
    "    grid_GS_v.ny,\n",
    "    size(untrained_CNODE_solution, 2),\n",
    "    :);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the results"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "anim = Animation()\n",
    "fig = plot(layout = (2, 6), size = (1200, 400))\n",
    "@gif for i in 1:1:size(u_trained, 4)\n",
    "    # First row: set of parameters 1\n",
    "    p1 = heatmap(u[:, :, 1, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :reds,\n",
    "        title = \"Exact\")\n",
    "    p2 = heatmap(v[:, :, 1, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :blues)\n",
    "    p3 = heatmap(u_untrained[:, :, 1, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :reds,\n",
    "        title = \"Untrained\")\n",
    "    p4 = heatmap(v_untrained[:, :, 1, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :blues)\n",
    "    p5 = heatmap(u_trained[:, :, 1, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :reds,\n",
    "        title = \"Trained\")\n",
    "    p6 = heatmap(v_trained[:, :, 1, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :blues)\n",
    "\n",
    "    # Second row: set of parameters 2\n",
    "    p7 = heatmap(u[:, :, 2, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :reds,\n",
    "        title = \"Exact\")\n",
    "    p8 = heatmap(v[:, :, 2, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :blues)\n",
    "    p9 = heatmap(u_untrained[:, :, 2, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :reds,\n",
    "        title = \"Untrained\")\n",
    "    p10 = heatmap(v_untrained[:, :, 2, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :blues)\n",
    "    p11 = heatmap(u_trained[:, :, 2, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :reds,\n",
    "        title = \"Trained\")\n",
    "    p12 = heatmap(v_trained[:, :, 2, i],\n",
    "        axis = false,\n",
    "        cbar = false,\n",
    "        aspect_ratio = 1,\n",
    "        color = :blues)\n",
    "\n",
    "    fig = plot(p1,\n",
    "        p2,\n",
    "        p3,\n",
    "        p4,\n",
    "        p5,\n",
    "        p6,\n",
    "        p7,\n",
    "        p8,\n",
    "        p9,\n",
    "        p10,\n",
    "        p11,\n",
    "        p12,\n",
    "        layout = (2, 6),\n",
    "        margin = 0mm)\n",
    "    frame(anim, fig)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the generated .gif"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if isdir(\"./plots\")\n",
    "    gif(anim, \"./plots/02.01-trained_GS.gif\", fps = 10)\n",
    "else\n",
    "    gif(anim, \"examples/plots/02.01-trained_GS.gif\", fps = 10)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that even with a posteriori loss of the order of 1e-7 still produces a different dynamics over time!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "myloss(θ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
